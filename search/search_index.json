{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Habit explorer Introduction : Habit Explorer is a marketing intelligence tool and is targeted for digital marketers. This tool will help the marketing teams understand and perform AI driven campaign activities. Habitual AI is the world's first AI engine that automates the process of building the habits. This patent pending Technology enables banks and financial institutions to boost habitual usage of their digital and mobile banking platforms. It learns how customers use a digital banking product and provides personalized features recommendations that put them on the path of long-term loyalty and habitualusage. By building customers habits, Habitual.AI delivered 145% increase in transaction, 45% reduction in customer churn and 75% increase in users who made 4 or more transaction a month. This interactive demo is design to simulate the feature recommendations that Habitual.AI is capable of generating for your unique customer base.","title":"Home"},{"location":"#welcome-to-habit-explorer","text":"Introduction : Habit Explorer is a marketing intelligence tool and is targeted for digital marketers. This tool will help the marketing teams understand and perform AI driven campaign activities. Habitual AI is the world's first AI engine that automates the process of building the habits. This patent pending Technology enables banks and financial institutions to boost habitual usage of their digital and mobile banking platforms. It learns how customers use a digital banking product and provides personalized features recommendations that put them on the path of long-term loyalty and habitualusage. By building customers habits, Habitual.AI delivered 145% increase in transaction, 45% reduction in customer churn and 75% increase in users who made 4 or more transaction a month. This interactive demo is design to simulate the feature recommendations that Habitual.AI is capable of generating for your unique customer base.","title":"Welcome to Habit explorer"},{"location":"HE_Software_Requirement/","text":"Product Function: The organizations that are using Habit Explorer for boosting habitual usage of their products can create apps (like Credit Card, Digital Banking, etc..) depending upon their requirements with required events. Apps works with the specific models (models are Machine learning Algorithms) to generate the Recommendations. Under the specific app a user can create any number of personas, Personas are specific group of customers of that particular organization. For example, the customers with age group less than 30 and the customers who are with that organization at least 6 months can be made as a persona. Personas contains several other attributes which we discuss later (3.2). For a persona user can select the events which were made by the persona so that habit path can be generated based on the events that are performed bypersona. Modules: 1. Apps Controller 2. Persona Controllers 3. Events Controllers 4. Insights Controllers Apps Controller : Apps controller contains all CURD operations related to apps that are created.Apps related information will be stored in apps table in DB. Apps depend upon the specific models to give required recommendations. EX: Digital Banking, Credit Card etc... Apps can be grouped, so that permissions for accessing the app given based on the user group ID. Apps groups information available in the app group table. Persona Controllers: Personas are specific group of customers of the organization. Persona Controller contains all CURD operations related to personas. Required number of personas can be created for the app depending upon the requirement by the user. Personas will have the persona attributes. Depending upon the app the persona attributes may changes. Personas information will be stored in persona table with selected persona attribute. And the which user created the persona with persona ID will store in User Perona Table. For example persona attributes canbe,vintage of the customers(means particular period of months or years customer stayed with the organization), Account type (savings or current), Digital Activity (active or inactive), gender (male orfemale. Event Controllers: Once the user created the persona,user can add the events made by the persona(group of customers of particular characteristics). Events for the app will be available in events table, where the selected events will be stored in persona eventstable. Events are the particular operation done by the persona like Digital payment done through UPI, Electricity Bill Payment, Mobile Bill Payment, savings, loans payments etc... Events depend upon apps. Different apps contain different events. Once the events where added by the users the recommended habit path is generated. Selected persona attributes with selected events and non-selected events was passed to the model. Models gives the probability of each event to be recommended.Were commend the events for which habitual score increases the maximum by performing that particularevents. Once the habitual score reaches the maximum means where by performing any event habitual score does not increases then that persona is said to be Habitual User. Habitual Score: Habitual AI model Persona Insights: Performance Parameter Models Software Requirement Spark, Scala,Python Data requirements User wise transaction history with there commendation for atleast 24months. Pre-Post data for 6 months (3 months pre \u2013 campaign \u2013 3 post period). Pre period data is kept similar to Pre period data on which Habitual AI model has been trained on. It could also be 1-month pre \u2013 campaign \u2013 1-monthpost. Conversion, retention and consistency metric have been trained on post period of 1, 2, 3 months respectively on a preperiodof one month. (These requirements are fixed from business point ofview). Event recommendations will be from a set of events, on which we train the habitualmodel. Keep note of event_l1_id and event_l2_id used in variousbanks. Habit score output (user_id, event_name) for each user who were recommended a feature. (from Habitual AImodel) Constraints The input data must be in Octopus format or need to be converted in required format. Only the debit events and digital events are considered as features, and the model is trained only for salaried account users. (This constraint can be modified depending on the requirements from thebanks). The habitual AI model generates recommendation for each user, but due to campaigning constraints (which includes cost and time/ DND services) we do not send a recommendation to each user. So, werunour model only on the users who were recommended afeature. The attribute names for different banks is different. For working on a single code, a pre-processing need to be done to get data in the sameformat. Assumptions All the calculation for performance parameters is calculated based on definitions provided in section below (Calculation of performance/campaign insights for HabitExplorer). Risks and Contingencies The hypothesis of expected conversion rate correlating positively with Habitual Score might not project correct results(this may be due to data or the trend) leading in drop of the conversion rate fromUI. All the models are trained independently, and might show little or no correlation.(This needs to be fixed in futurerelease). Data mining goals To project the expected conversion rate,we need to model the user behaviour to predict the conversion rate. We plan to use a regression (linear,non-linear) model to accurately predict the conversionrate. Predictive Model : We will create an ML model such as regression which will predict the conversion rate. The input training data X (user demographics and events which the user has done in the past (pre-period), recommendation from Habitual AI model) and target data Y will be created from pre-post data following the parameter definitions given below. (in section performance parameters). Initally, the X label (or features) were created for a single user as pivoting all events and create black and white features (Whether userhas donethe event or not), Recommendation from Habitual AI and user demographics (which might be one-hot encoded or labelled feature) or continuous features. This implied foreachuser a single feature vector is created which didn't incorporate the trend of the user for each event.Hence,for BancoPAN we came up with following idea. X label generation: For every user, a path history is created, give the assumption that a user is recommended one of the features (from the merchant category). This means that for each user at most 22 paths) can be generated, which shows how the behavior changes if he/she does a particular feature. Other than merchant features, the persona of the user is also considered into account (age, gender, global_limit, card_type, home_state). Here R, is the recommended feature. (Internal Id is only for mapping purpose, won't be considered as a feature.). All the features performed by the users are in the pre period (a month before the campaign_date, inclusive). Feature mappings :: Click here Digital Activity: -- The target will be the ratio of digital transactions to the total transactions in the post period by the user. Spend Activation: Average transaction done by the user. Variety : Number of distinct events performed by a user in post period (one month from campaign date).(For future release this can be projected as buckets, which may reduce the error) Engagement : Number of events performed by a user in post period (one month from campaign date).(For future release this can be projected as buckets, which may reduce the error) Conversion : True, if a user performs a given recommendation in the post period (one month from campaign date). The target is the probability ofconversion. Retention : True, givenifa user is converted and inthesubsequent two months he performs the same event as recommended in at least either of themonth. Consistency : True, if a user is converted and performs the same event for subsequent two months. Mlflow Documentation : Click here Hands on for mlflow: Click here Insight Controllers: These insights are used to get the metrics (spends and volumes) of the credit card transactions for each persona created by the user By Calendar Insights: Calendar insights are to display the spends and volumes of the month wise transactions in a heat map. By Trends Insights: Trend insights are to display the spends and unique users of the selected month wise transactions of the selected categories sorted by rank.","title":"Functional Specifications"},{"location":"HE_Software_Requirement/#software-requirement","text":"Spark, Scala,Python","title":"Software Requirement"},{"location":"HE_Software_Requirement/#data-requirements","text":"User wise transaction history with there commendation for atleast 24months. Pre-Post data for 6 months (3 months pre \u2013 campaign \u2013 3 post period). Pre period data is kept similar to Pre period data on which Habitual AI model has been trained on. It could also be 1-month pre \u2013 campaign \u2013 1-monthpost. Conversion, retention and consistency metric have been trained on post period of 1, 2, 3 months respectively on a preperiodof one month. (These requirements are fixed from business point ofview). Event recommendations will be from a set of events, on which we train the habitualmodel. Keep note of event_l1_id and event_l2_id used in variousbanks. Habit score output (user_id, event_name) for each user who were recommended a feature. (from Habitual AImodel)","title":"Data requirements"},{"location":"HE_Software_Requirement/#constraints","text":"The input data must be in Octopus format or need to be converted in required format. Only the debit events and digital events are considered as features, and the model is trained only for salaried account users. (This constraint can be modified depending on the requirements from thebanks). The habitual AI model generates recommendation for each user, but due to campaigning constraints (which includes cost and time/ DND services) we do not send a recommendation to each user. So, werunour model only on the users who were recommended afeature. The attribute names for different banks is different. For working on a single code, a pre-processing need to be done to get data in the sameformat.","title":"Constraints"},{"location":"HE_Software_Requirement/#assumptions","text":"All the calculation for performance parameters is calculated based on definitions provided in section below (Calculation of performance/campaign insights for HabitExplorer).","title":"Assumptions"},{"location":"HE_Software_Requirement/#risks-and-contingencies","text":"The hypothesis of expected conversion rate correlating positively with Habitual Score might not project correct results(this may be due to data or the trend) leading in drop of the conversion rate fromUI. All the models are trained independently, and might show little or no correlation.(This needs to be fixed in futurerelease).","title":"Risks and Contingencies"},{"location":"HE_Software_Requirement/#data-mining-goals","text":"To project the expected conversion rate,we need to model the user behaviour to predict the conversion rate. We plan to use a regression (linear,non-linear) model to accurately predict the conversionrate. Predictive Model : We will create an ML model such as regression which will predict the conversion rate. The input training data X (user demographics and events which the user has done in the past (pre-period), recommendation from Habitual AI model) and target data Y will be created from pre-post data following the parameter definitions given below. (in section performance parameters). Initally, the X label (or features) were created for a single user as pivoting all events and create black and white features (Whether userhas donethe event or not), Recommendation from Habitual AI and user demographics (which might be one-hot encoded or labelled feature) or continuous features. This implied foreachuser a single feature vector is created which didn't incorporate the trend of the user for each event.Hence,for BancoPAN we came up with following idea. X label generation: For every user, a path history is created, give the assumption that a user is recommended one of the features (from the merchant category). This means that for each user at most 22 paths) can be generated, which shows how the behavior changes if he/she does a particular feature. Other than merchant features, the persona of the user is also considered into account (age, gender, global_limit, card_type, home_state). Here R, is the recommended feature. (Internal Id is only for mapping purpose, won't be considered as a feature.). All the features performed by the users are in the pre period (a month before the campaign_date, inclusive). Feature mappings :: Click here Digital Activity: -- The target will be the ratio of digital transactions to the total transactions in the post period by the user. Spend Activation: Average transaction done by the user. Variety : Number of distinct events performed by a user in post period (one month from campaign date).(For future release this can be projected as buckets, which may reduce the error) Engagement : Number of events performed by a user in post period (one month from campaign date).(For future release this can be projected as buckets, which may reduce the error) Conversion : True, if a user performs a given recommendation in the post period (one month from campaign date). The target is the probability ofconversion. Retention : True, givenifa user is converted and inthesubsequent two months he performs the same event as recommended in at least either of themonth. Consistency : True, if a user is converted and performs the same event for subsequent two months. Mlflow Documentation : Click here Hands on for mlflow: Click here Insight Controllers: These insights are used to get the metrics (spends and volumes) of the credit card transactions for each persona created by the user By Calendar Insights: Calendar insights are to display the spends and volumes of the month wise transactions in a heat map. By Trends Insights: Trend insights are to display the spends and unique users of the selected month wise transactions of the selected categories sorted by rank.","title":"Data mining goals"},{"location":"Installation/","text":"Installation/Usage If you wish to run your own build, first ensure you have python3 globally installed in your computer. If not, you can get python3 here . After this, ensure you have installed virtualenv globally as well. If not, run this: $ pip install virtualenv Git clone this repo to your PC $ git clone https://github.com/3LOQ/habit-explorer.git - Create and fire up your virtual environment in python3 $ virtualenv -p python3 venv $ source venv/bin/activate Install Dependencies 1. Cd into your the cloned repo as such: ``` (venv)$ cd habit-explorer ``` 2. Install your requirements ``` (venv)$ pip install -r requirements.txt ``` Running On your terminal, run the server using this one simple command: ``` (venv)$ python service.py ``` You can now access the app on your local browser by using ``` http://localhost:9999/swagger/ ``` Testing On your terminal, run the following command to check results of your unit test cases ``` (venv)$ mvn clean test -D test='ApiTestRunner' -D karate.env='custom' -D karate.username='admin' -D karate.password='syncfusion' -D karate.url='[http://localhost:9999](http://localhost:9999/)' ```","title":"Installation/Usage"},{"location":"Installation/#installationusage","text":"If you wish to run your own build, first ensure you have python3 globally installed in your computer. If not, you can get python3 here . After this, ensure you have installed virtualenv globally as well. If not, run this: $ pip install virtualenv Git clone this repo to your PC $ git clone https://github.com/3LOQ/habit-explorer.git - Create and fire up your virtual environment in python3 $ virtualenv -p python3 venv $ source venv/bin/activate","title":"Installation/Usage"},{"location":"Installation/#install-dependencies","text":"1. Cd into your the cloned repo as such: ``` (venv)$ cd habit-explorer ``` 2. Install your requirements ``` (venv)$ pip install -r requirements.txt ```","title":"Install Dependencies"},{"location":"Installation/#running","text":"On your terminal, run the server using this one simple command: ``` (venv)$ python service.py ``` You can now access the app on your local browser by using ``` http://localhost:9999/swagger/ ```","title":"Running"},{"location":"Installation/#testing","text":"On your terminal, run the following command to check results of your unit test cases ``` (venv)$ mvn clean test -D test='ApiTestRunner' -D karate.env='custom' -D karate.username='admin' -D karate.password='syncfusion' -D karate.url='[http://localhost:9999](http://localhost:9999/)' ```","title":"Testing"},{"location":"Installations_Usage/","text":"Installation/Usage If you wish to run your own build, first ensure you have python3 globally installed in your computer. If not, you can get python3 here . After this, ensure you have installed virtualenv globally as well. If not, run this: $ pip install virtualenv Git clone this repo to your PC $ git clone https://github.com/3LOQ/habit-explorer.git - Create and fire up your virtual environment in python3 $ virtualenv -p python3 venv $ source venv/bin/activate - #### Install Dependencies Cd into your the cloned repo as such: (venv)$ cd habit-explorer Install your requirements (venv)$ pip install -r requirements.txt Running On your terminal, run the server using this one simple command: (venv)$ python service.py You can now access the app on your local browser by using http://localhost:9999/swagger/ Testing On your terminal, run the following command to check results of your unit test cases ``` (venv)$ mvn clean test -D test='ApiTestRunner' -D karate.env='custom' -D karate.username='admin' -D karate.password='syncfusion' -D karate.url=' http://localhost:9999 '","title":"Installations Usage"},{"location":"Installations_Usage/#installationusage","text":"If you wish to run your own build, first ensure you have python3 globally installed in your computer. If not, you can get python3 here . After this, ensure you have installed virtualenv globally as well. If not, run this: $ pip install virtualenv Git clone this repo to your PC $ git clone https://github.com/3LOQ/habit-explorer.git - Create and fire up your virtual environment in python3 $ virtualenv -p python3 venv $ source venv/bin/activate - #### Install Dependencies Cd into your the cloned repo as such: (venv)$ cd habit-explorer Install your requirements (venv)$ pip install -r requirements.txt","title":"Installation/Usage"},{"location":"Installations_Usage/#running","text":"On your terminal, run the server using this one simple command: (venv)$ python service.py You can now access the app on your local browser by using http://localhost:9999/swagger/","title":"Running"},{"location":"Installations_Usage/#testing","text":"On your terminal, run the following command to check results of your unit test cases ``` (venv)$ mvn clean test -D test='ApiTestRunner' -D karate.env='custom' -D karate.username='admin' -D karate.password='syncfusion' -D karate.url=' http://localhost:9999 '","title":"Testing"},{"location":"Technologies_Used/","text":"Technologies Used Python3 - A programming language that lets you work more quickly (The universe loves speed!). Flask - A microframework for Python based on Werkzeug, Jinja 2 and good intentions Virtualenv - A tool to create isolated virtual environments PostgreSQL \u2013 Postgres database offers many advantages over others. Minor dependencies can be found in the requirements.txt file on the root folder.","title":"Technologies Used"},{"location":"Technologies_Used/#technologies-used","text":"Python3 - A programming language that lets you work more quickly (The universe loves speed!). Flask - A microframework for Python based on Werkzeug, Jinja 2 and good intentions Virtualenv - A tool to create isolated virtual environments PostgreSQL \u2013 Postgres database offers many advantages over others. Minor dependencies can be found in the requirements.txt file on the root folder.","title":"Technologies Used"}]}